{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T20:59:28.339023Z",
     "start_time": "2019-02-06T20:59:23.952846Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "sys.path.extend(['..'])\n",
    "\n",
    "from utils.config import process_config\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.layers import (conv2d, max_pooling2d, average_pooling2d, batch_normalization, dropout, dense)\n",
    "from tensorflow.nn import (relu, sigmoid, softmax, leaky_relu)\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T20:59:28.345740Z",
     "start_time": "2019-02-06T20:59:28.342044Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA = '../data/data_clean/'\n",
    "CONF = '../configs/roman.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T20:59:28.388136Z",
     "start_time": "2019-02-06T20:59:28.382427Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    return (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "def shuffle_sim(a, b):\n",
    "    assert a.shape[0] == a.shape[0], 'Shapes must be equal'\n",
    "    \n",
    "    ind = np.arange(a.shape[0])\n",
    "    np.random.shuffle(ind)\n",
    "    return a[ind], b[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T20:59:28.777358Z",
     "start_time": "2019-02-06T20:59:28.767622Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_train_test(path_to_data):\n",
    "    data = {}\n",
    "    for dset in ['train', 'test']:\n",
    "        path_ = os.path.join(path_to_data, dset)\n",
    "        X, Y = [], []\n",
    "        classes = [d for d in os.listdir(path_) if os.path.isdir(os.path.join(path_, d))]\n",
    "        classes.sort()\n",
    "        \n",
    "        for cl in classes:\n",
    "            y = np.zeros((1, 8), dtype=np.int32)\n",
    "            y[0, int(cl) - 1] = 1\n",
    "            \n",
    "            cl_path = os.path.join(path_, cl)\n",
    "            filenames = [os.path.join(cl_path, pict) for pict in os.listdir(cl_path) if pict.endswith('.jpg')]\n",
    "            \n",
    "            for im in filenames:\n",
    "                image = np.asarray(Image.open(im), dtype=np.float32)\n",
    "                X.append(normalize(image).reshape((1, image.shape[0], image.shape[1], image.shape[2])))\n",
    "                Y.append(y)\n",
    "        \n",
    "        a, b = shuffle_sim(np.concatenate(X), np.concatenate(Y))\n",
    "        data[dset] = ([a, b])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T20:59:29.759131Z",
     "start_time": "2019-02-06T20:59:29.649326Z"
    },
    "code_folding": [
     2,
     18,
     53,
     88,
     106,
     114
    ]
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self, config, sess_cf, learning_rate):\n",
    "        self.lr = learning_rate\n",
    "        self.sess = tf.Session(config=sess_cf)\n",
    "\n",
    "        self.x = tf.placeholder(dtype=tf.float32, shape=(None, config.image_size, config.image_size, 3))\n",
    "        self.y = tf.placeholder(dtype=tf.int32, shape=(None, 8))\n",
    "        self.training = tf.placeholder(dtype=tf.bool, shape=())\n",
    "\n",
    "        global_step = tf.Variable(1, name='global_step', trainable=False, dtype=tf.int32)\n",
    "        self.step = tf.assign(global_step, global_step + 1)\n",
    "        \n",
    "        self.model()\n",
    "        \n",
    "        self.summ_writer = tf.summary.FileWriter(config.summary_dir, graph=self.sess.graph)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def block(self, inp,\n",
    "              ch,\n",
    "              num,\n",
    "              c_ker=[(3, 3), (3, 3)],\n",
    "              c_str=[(1, 1), (1, 1)],\n",
    "              act=relu,\n",
    "              mp_ker=(2, 2),\n",
    "              mp_str=(2, 2),\n",
    "              mode='conc'):\n",
    "    \n",
    "        with tf.variable_scope('block_' + str(num), reuse=tf.AUTO_REUSE):\n",
    "            conv = conv2d(inp, ch, c_ker[0], strides=c_str[0])\n",
    "            bn = batch_normalization(conv)\n",
    "            out = act(bn)\n",
    "            out = dropout(out, 0.2)\n",
    "            tf.summary.histogram('conv1', conv)\n",
    "            print(out.shape)\n",
    "            \n",
    "            conv = conv2d(out, ch, c_ker[1], strides=c_str[1])\n",
    "            bn = batch_normalization(conv)\n",
    "            out = act(bn)\n",
    "            tf.summary.histogram('conv2', conv)\n",
    "            print(out.shape)\n",
    "            \n",
    "            mp = max_pooling2d(out, mp_ker, strides=mp_str)\n",
    "            if mode == 'mp':\n",
    "                out = mp\n",
    "            elif mode == 'conc':\n",
    "                out = tf.concat([mp, average_pooling2d(out, mp_ker, mp_str)], -1)\n",
    "            else:\n",
    "                raise ValueError('Unknown mode.')\n",
    "            \n",
    "            print(out.shape)\n",
    "        return out\n",
    "    \n",
    "    def model(self):\n",
    "        with tf.name_scope('layers'):\n",
    "            out = self.block(self.x, 16, 1, mode='conc')\n",
    "            out = self.block(out, 32, 2, mode='conc')\n",
    "            out = self.block(out, 64, 3, mode='conc')\n",
    "            out = self.block(out, 256, 4, c_str=[(1, 1), (2, 2)], mode='mp')\n",
    "            \n",
    "            dim = np.prod(out.shape[1:])\n",
    "            out = tf.reshape(out, [-1, dim])\n",
    "            print(out.shape)\n",
    "            \n",
    "            dense_l = dense(out, 128)\n",
    "            tf.summary.histogram('dense2', dense_l)\n",
    "            out = batch_normalization(dense_l)\n",
    "            out = leaky_relu(out, alpha=0.01)\n",
    "            out = dropout(out, rate=0.7, training=self.training)\n",
    "            print(out.shape)\n",
    "\n",
    "            self.predictions = dense(out, 8, activation=softmax)\n",
    "            tf.summary.histogram('pred', self.predictions)\n",
    "\n",
    "        with tf.name_scope('metrics'):    \n",
    "            amax_labels = tf.argmax(self.y, 1)\n",
    "            amax_pred   = tf.argmax(self.predictions, 1)\n",
    "\n",
    "            self.loss = tf.losses.softmax_cross_entropy(self.y, self.predictions)        \n",
    "            self.acc = tf.reduce_mean(tf.cast(tf.equal(amax_labels, amax_pred), dtype=tf.float32))\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            tf.summary.scalar('accuracy', self.acc)\n",
    "\n",
    "        self.summary = tf.summary.merge_all()\n",
    "        \n",
    "    def train(self, dat, dat_v, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            loss, acc, _, summary, step = self.sess.run([\n",
    "                self.loss, self.acc, self.optimizer, self.summary, self.step\n",
    "            ],\n",
    "                                                feed_dict={\n",
    "                                                    self.x: dat[0],\n",
    "                                                    self.y: dat[1],\n",
    "                                                    self.training: True\n",
    "                                                })\n",
    "\n",
    "            self.summ_writer.add_summary(summary, step)\n",
    "            print('EP: {:3d}\\tLOSS: {:.10f}\\tACC: {:.10f}'.format(\n",
    "                epoch, loss, acc))\n",
    "\n",
    "            if epoch % 10 == 0 and epoch != 0:\n",
    "                self.test(dat_v)\n",
    "                \n",
    "    def test(self, dat):\n",
    "        loss, acc = self.sess.run([self.loss, self.acc],\n",
    "                                         feed_dict={self.x: dat[0],\n",
    "                                                    self.y: dat[1],\n",
    "                                                    self.training: False})\n",
    "\n",
    "        print('\\tVALIDATION\\tLOSS: {:.10f}\\tACC: {:.10f}'.format(loss, acc))\n",
    "    \n",
    "    def close(self):\n",
    "        self.sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T18:10:51.511900Z",
     "start_time": "2019-02-06T18:10:51.497555Z"
    }
   },
   "outputs": [],
   "source": [
    "m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T20:59:45.797270Z",
     "start_time": "2019-02-06T20:59:45.793167Z"
    }
   },
   "outputs": [],
   "source": [
    "config_tf = tf.ConfigProto(allow_soft_placement=True)\n",
    "config_tf.gpu_options.allow_growth = True\n",
    "config_tf.gpu_options.per_process_gpu_memory_fraction = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T21:36:29.692442Z",
     "start_time": "2019-02-06T21:36:29.688827Z"
    }
   },
   "outputs": [],
   "source": [
    "config = process_config(CONF)\n",
    "config['exp_name'] = '4b_clean_data'\n",
    "config['summary_dir'] = '../experiments/' + config['exp_name'] + '/summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T21:36:39.367384Z",
     "start_time": "2019-02-06T21:36:39.354891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = process_config(CONF)\n",
    "im_size = config.image_size\n",
    "# im_size = config['image_size'] is also works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T20:59:52.427774Z",
     "start_time": "2019-02-06T20:59:47.969654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 126, 126, 16)\n",
      "(?, 124, 124, 16)\n",
      "(?, 62, 62, 32)\n",
      "(?, 60, 60, 32)\n",
      "(?, 58, 58, 32)\n",
      "(?, 29, 29, 64)\n",
      "(?, 27, 27, 64)\n",
      "(?, 25, 25, 64)\n",
      "(?, 12, 12, 128)\n",
      "(?, 10, 10, 256)\n",
      "(?, 4, 4, 256)\n",
      "(?, 2, 2, 256)\n",
      "(?, 1024)\n",
      "(?, 128)\n"
     ]
    }
   ],
   "source": [
    "m = Model(config, config_tf, 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T20:59:53.861162Z",
     "start_time": "2019-02-06T20:59:52.836865Z"
    }
   },
   "outputs": [],
   "source": [
    "dat = read_train_test(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T21:34:38.649243Z",
     "start_time": "2019-02-06T20:59:58.152366Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP:   0\tLOSS: 2.0792574883\tACC: 0.1280276775\n",
      "EP:   1\tLOSS: 2.0785179138\tACC: 0.1314878911\n",
      "EP:   2\tLOSS: 2.0783586502\tACC: 0.1332179904\n",
      "EP:   3\tLOSS: 2.0800726414\tACC: 0.1435986161\n",
      "EP:   4\tLOSS: 2.0764451027\tACC: 0.1418685168\n",
      "EP:   5\tLOSS: 2.0783786774\tACC: 0.1418685168\n",
      "EP:   6\tLOSS: 2.0789024830\tACC: 0.1314878911\n",
      "EP:   7\tLOSS: 2.0771849155\tACC: 0.1522491276\n",
      "EP:   8\tLOSS: 2.0755751133\tACC: 0.1349480897\n",
      "EP:   9\tLOSS: 2.0766043663\tACC: 0.1211072654\n",
      "EP:  10\tLOSS: 2.0753393173\tACC: 0.1349480897\n",
      "\tVALIDATION\tLOSS: 2.0752797127\tACC: 0.1320754737\n",
      "EP:  11\tLOSS: 2.0745828152\tACC: 0.1384083033\n",
      "EP:  12\tLOSS: 2.0780797005\tACC: 0.1435986161\n",
      "EP:  13\tLOSS: 2.0744574070\tACC: 0.1505190283\n",
      "EP:  14\tLOSS: 2.0698380470\tACC: 0.1522491276\n",
      "EP:  15\tLOSS: 2.0729060173\tACC: 0.1487889290\n",
      "EP:  16\tLOSS: 2.0708818436\tACC: 0.1435986161\n",
      "EP:  17\tLOSS: 2.0754349232\tACC: 0.1366782039\n",
      "EP:  18\tLOSS: 2.0685412884\tACC: 0.1297577918\n",
      "EP:  19\tLOSS: 2.0717496872\tACC: 0.1349480897\n",
      "EP:  20\tLOSS: 2.0705764294\tACC: 0.1332179904\n",
      "\tVALIDATION\tLOSS: 2.0647232533\tACC: 0.1320754737\n",
      "EP:  21\tLOSS: 2.0645201206\tACC: 0.1539792418\n",
      "EP:  22\tLOSS: 2.0600483418\tACC: 0.1678200662\n",
      "EP:  23\tLOSS: 2.0520374775\tACC: 0.1608996540\n",
      "EP:  24\tLOSS: 2.0544223785\tACC: 0.1557093412\n",
      "EP:  25\tLOSS: 2.0447196960\tACC: 0.1868512183\n",
      "EP:  26\tLOSS: 2.0407371521\tACC: 0.1972318292\n",
      "EP:  27\tLOSS: 2.0425944328\tACC: 0.1799307913\n",
      "EP:  28\tLOSS: 2.0340528488\tACC: 0.1920415163\n",
      "EP:  29\tLOSS: 2.0276439190\tACC: 0.2508650422\n",
      "EP:  30\tLOSS: 2.0032584667\tACC: 0.2993079722\n",
      "\tVALIDATION\tLOSS: 1.9918373823\tACC: 0.2830188572\n",
      "EP:  31\tLOSS: 2.0067682266\tACC: 0.2525951564\n",
      "EP:  32\tLOSS: 1.9770575762\tACC: 0.2889273465\n",
      "EP:  33\tLOSS: 1.9737671614\tACC: 0.2854671180\n",
      "EP:  34\tLOSS: 1.9322495461\tACC: 0.3287197351\n",
      "EP:  35\tLOSS: 1.9079918861\tACC: 0.3598615825\n",
      "EP:  36\tLOSS: 1.9441285133\tACC: 0.3096885681\n",
      "EP:  37\tLOSS: 1.9308247566\tACC: 0.3287197351\n",
      "EP:  38\tLOSS: 1.9101539850\tACC: 0.3564013839\n",
      "EP:  39\tLOSS: 1.9316995144\tACC: 0.3235294223\n",
      "EP:  40\tLOSS: 1.8971680403\tACC: 0.3702422082\n",
      "\tVALIDATION\tLOSS: 1.9039410353\tACC: 0.3490566015\n",
      "EP:  41\tLOSS: 1.9138200283\tACC: 0.3408304453\n",
      "EP:  42\tLOSS: 1.8756062984\tACC: 0.3858131468\n",
      "EP:  43\tLOSS: 1.8723706007\tACC: 0.3892733455\n",
      "EP:  44\tLOSS: 1.8615553379\tACC: 0.3996539712\n",
      "EP:  45\tLOSS: 1.8564316034\tACC: 0.4117647111\n",
      "EP:  46\tLOSS: 1.8683058023\tACC: 0.4031141996\n",
      "EP:  47\tLOSS: 1.8582719564\tACC: 0.4117647111\n",
      "EP:  48\tLOSS: 1.8331797123\tACC: 0.4256055355\n",
      "EP:  49\tLOSS: 1.8385745287\tACC: 0.4359861612\n",
      "EP:  50\tLOSS: 1.8708643913\tACC: 0.3996539712\n",
      "\tVALIDATION\tLOSS: 1.8058977127\tACC: 0.4669811428\n",
      "EP:  51\tLOSS: 1.8343769312\tACC: 0.4307958484\n",
      "EP:  52\tLOSS: 1.8168417215\tACC: 0.4480968714\n",
      "EP:  53\tLOSS: 1.8492685556\tACC: 0.4100345969\n",
      "EP:  54\tLOSS: 1.8048027754\tACC: 0.4671280384\n",
      "EP:  55\tLOSS: 1.8137608767\tACC: 0.4532871842\n",
      "EP:  56\tLOSS: 1.8258407116\tACC: 0.4446366727\n",
      "EP:  57\tLOSS: 1.8114665747\tACC: 0.4602076113\n",
      "EP:  58\tLOSS: 1.8119616508\tACC: 0.4602076113\n",
      "EP:  59\tLOSS: 1.7927727699\tACC: 0.4775086641\n",
      "EP:  60\tLOSS: 1.7963856459\tACC: 0.4826989472\n",
      "\tVALIDATION\tLOSS: 1.7544610500\tACC: 0.5283018947\n",
      "EP:  61\tLOSS: 1.7834719419\tACC: 0.4723183513\n",
      "EP:  62\tLOSS: 1.7708778381\tACC: 0.5000000000\n",
      "EP:  63\tLOSS: 1.7688530684\tACC: 0.5069203973\n",
      "EP:  64\tLOSS: 1.7745791674\tACC: 0.5017300844\n",
      "EP:  65\tLOSS: 1.7731528282\tACC: 0.5000000000\n",
      "EP:  66\tLOSS: 1.7546843290\tACC: 0.5259515643\n",
      "EP:  67\tLOSS: 1.7406851053\tACC: 0.5311418772\n",
      "EP:  68\tLOSS: 1.7281728983\tACC: 0.5501729846\n",
      "EP:  69\tLOSS: 1.7226955891\tACC: 0.5467128158\n",
      "EP:  70\tLOSS: 1.7170182467\tACC: 0.5519031286\n",
      "\tVALIDATION\tLOSS: 1.6922702789\tACC: 0.5754716992\n",
      "EP:  71\tLOSS: 1.7048480511\tACC: 0.5622837543\n",
      "EP:  72\tLOSS: 1.7027925253\tACC: 0.5622837543\n",
      "EP:  73\tLOSS: 1.7151806355\tACC: 0.5484429002\n",
      "EP:  74\tLOSS: 1.6932808161\tACC: 0.5743944645\n",
      "EP:  75\tLOSS: 1.6921874285\tACC: 0.5778546929\n",
      "EP:  76\tLOSS: 1.6898189783\tACC: 0.5761245489\n",
      "EP:  77\tLOSS: 1.6709400415\tACC: 0.6003460288\n",
      "EP:  78\tLOSS: 1.6647375822\tACC: 0.6176470518\n",
      "EP:  79\tLOSS: 1.6752599478\tACC: 0.5899654031\n",
      "EP:  80\tLOSS: 1.6588621140\tACC: 0.6262975931\n",
      "\tVALIDATION\tLOSS: 1.6021649837\tACC: 0.6886792183\n",
      "EP:  81\tLOSS: 1.6565456390\tACC: 0.6141868234\n",
      "EP:  82\tLOSS: 1.6629729271\tACC: 0.6020761132\n",
      "EP:  83\tLOSS: 1.6500221491\tACC: 0.6193771362\n",
      "EP:  84\tLOSS: 1.6566410065\tACC: 0.6211072803\n",
      "EP:  85\tLOSS: 1.6341594458\tACC: 0.6366782188\n",
      "EP:  86\tLOSS: 1.6208343506\tACC: 0.6608996391\n",
      "EP:  87\tLOSS: 1.6178606749\tACC: 0.6522491574\n",
      "EP:  88\tLOSS: 1.6406302452\tACC: 0.6384083033\n",
      "EP:  89\tLOSS: 1.6476025581\tACC: 0.6228373647\n",
      "EP:  90\tLOSS: 1.6115931273\tACC: 0.6747404933\n",
      "\tVALIDATION\tLOSS: 1.5609022379\tACC: 0.7216981053\n",
      "EP:  91\tLOSS: 1.6096978188\tACC: 0.6539792418\n",
      "EP:  92\tLOSS: 1.6073309183\tACC: 0.6695501804\n",
      "EP:  93\tLOSS: 1.6054378748\tACC: 0.6712802649\n",
      "EP:  94\tLOSS: 1.6084553003\tACC: 0.6643598676\n",
      "EP:  95\tLOSS: 1.5879499912\tACC: 0.6851211190\n",
      "EP:  96\tLOSS: 1.6105145216\tACC: 0.6643598676\n",
      "EP:  97\tLOSS: 1.6210283041\tACC: 0.6505190134\n",
      "EP:  98\tLOSS: 1.6100738049\tACC: 0.6522491574\n",
      "EP:  99\tLOSS: 1.5894556046\tACC: 0.6833909750\n",
      "EP: 100\tLOSS: 1.5861492157\tACC: 0.6885812879\n",
      "\tVALIDATION\tLOSS: 1.5275692940\tACC: 0.7405660152\n",
      "EP: 101\tLOSS: 1.5887372494\tACC: 0.6816608906\n",
      "EP: 102\tLOSS: 1.5836631060\tACC: 0.6799308062\n",
      "EP: 103\tLOSS: 1.5914717913\tACC: 0.6799308062\n",
      "EP: 104\tLOSS: 1.5966534615\tACC: 0.6730104089\n",
      "EP: 105\tLOSS: 1.5883679390\tACC: 0.6868512034\n",
      "EP: 106\tLOSS: 1.5703788996\tACC: 0.7006920576\n",
      "EP: 107\tLOSS: 1.5718607903\tACC: 0.6989619136\n",
      "EP: 108\tLOSS: 1.5978524685\tACC: 0.6695501804\n",
      "EP: 109\tLOSS: 1.5629966259\tACC: 0.7110726833\n",
      "EP: 110\tLOSS: 1.5539230108\tACC: 0.7197231650\n",
      "\tVALIDATION\tLOSS: 1.4809458256\tACC: 0.8066037893\n",
      "EP: 111\tLOSS: 1.5329656601\tACC: 0.7474048734\n",
      "EP: 112\tLOSS: 1.5588744879\tACC: 0.7076124549\n",
      "EP: 113\tLOSS: 1.5762232542\tACC: 0.6937716007\n",
      "EP: 114\tLOSS: 1.5576775074\tACC: 0.7179930806\n",
      "EP: 115\tLOSS: 1.5448474884\tACC: 0.7370242476\n",
      "EP: 116\tLOSS: 1.5046198368\tACC: 0.7768166065\n",
      "EP: 117\tLOSS: 1.5146070719\tACC: 0.7681660652\n",
      "EP: 118\tLOSS: 1.5469007492\tACC: 0.7283737063\n",
      "EP: 119\tLOSS: 1.5386887789\tACC: 0.7370242476\n",
      "EP: 120\tLOSS: 1.5466905832\tACC: 0.7266436219\n",
      "\tVALIDATION\tLOSS: 1.4944219589\tACC: 0.7783018947\n",
      "EP: 121\tLOSS: 1.5533467531\tACC: 0.7179930806\n",
      "EP: 122\tLOSS: 1.5351066589\tACC: 0.7422145605\n",
      "EP: 123\tLOSS: 1.4912747145\tACC: 0.7871972322\n",
      "EP: 124\tLOSS: 1.5032998323\tACC: 0.7733563781\n",
      "EP: 125\tLOSS: 1.5223960876\tACC: 0.7508650422\n",
      "EP: 126\tLOSS: 1.5105960369\tACC: 0.7629757524\n",
      "EP: 127\tLOSS: 1.5101256371\tACC: 0.7629757524\n",
      "EP: 128\tLOSS: 1.5186340809\tACC: 0.7474048734\n",
      "EP: 129\tLOSS: 1.5062446594\tACC: 0.7716262937\n",
      "EP: 130\tLOSS: 1.5110539198\tACC: 0.7577854395\n",
      "\tVALIDATION\tLOSS: 1.4523774385\tACC: 0.8207547069\n",
      "EP: 131\tLOSS: 1.5162991285\tACC: 0.7595155835\n",
      "EP: 132\tLOSS: 1.5083396435\tACC: 0.7698962092\n",
      "EP: 133\tLOSS: 1.4694159031\tACC: 0.8027681708\n",
      "EP: 134\tLOSS: 1.4963302612\tACC: 0.7785466909\n",
      "EP: 135\tLOSS: 1.4840090275\tACC: 0.7941176295\n",
      "EP: 136\tLOSS: 1.4806532860\tACC: 0.7923875451\n",
      "EP: 137\tLOSS: 1.4849513769\tACC: 0.7941176295\n",
      "EP: 138\tLOSS: 1.4773910046\tACC: 0.7958477736\n",
      "EP: 139\tLOSS: 1.5039546490\tACC: 0.7733563781\n",
      "EP: 140\tLOSS: 1.4922512770\tACC: 0.7837370038\n",
      "\tVALIDATION\tLOSS: 1.4347965717\tACC: 0.8443396091\n",
      "EP: 141\tLOSS: 1.4844633341\tACC: 0.7854671478\n",
      "EP: 142\tLOSS: 1.4867180586\tACC: 0.7837370038\n",
      "EP: 143\tLOSS: 1.4637633562\tACC: 0.8114187121\n",
      "EP: 144\tLOSS: 1.4656976461\tACC: 0.8096885681\n",
      "EP: 145\tLOSS: 1.4791393280\tACC: 0.7941176295\n",
      "EP: 146\tLOSS: 1.4615769386\tACC: 0.8166090250\n",
      "EP: 147\tLOSS: 1.4862381220\tACC: 0.7871972322\n",
      "EP: 148\tLOSS: 1.4740562439\tACC: 0.7958477736\n",
      "EP: 149\tLOSS: 1.4648996592\tACC: 0.8096885681\n",
      "EP: 150\tLOSS: 1.4643360376\tACC: 0.8148788810\n",
      "\tVALIDATION\tLOSS: 1.4477514029\tACC: 0.8301886916\n",
      "EP: 151\tLOSS: 1.4446979761\tACC: 0.8321799040\n",
      "EP: 152\tLOSS: 1.4683771133\tACC: 0.8062283993\n",
      "EP: 153\tLOSS: 1.4529838562\tACC: 0.8183391094\n",
      "EP: 154\tLOSS: 1.4801852703\tACC: 0.7923875451\n",
      "EP: 155\tLOSS: 1.4504195452\tACC: 0.8252595067\n",
      "EP: 156\tLOSS: 1.4519902468\tACC: 0.8235294223\n",
      "EP: 157\tLOSS: 1.4614194632\tACC: 0.8148788810\n",
      "EP: 158\tLOSS: 1.4550825357\tACC: 0.8183391094\n",
      "EP: 159\tLOSS: 1.4560796022\tACC: 0.8166090250\n",
      "EP: 160\tLOSS: 1.4398536682\tACC: 0.8356401324\n",
      "\tVALIDATION\tLOSS: 1.4285262823\tACC: 0.8443396091\n",
      "EP: 161\tLOSS: 1.4376742840\tACC: 0.8321799040\n",
      "EP: 162\tLOSS: 1.4503154755\tACC: 0.8304498196\n",
      "EP: 163\tLOSS: 1.4516812563\tACC: 0.8200691938\n",
      "EP: 164\tLOSS: 1.4414885044\tACC: 0.8321799040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP: 165\tLOSS: 1.4252028465\tACC: 0.8460207582\n",
      "EP: 166\tLOSS: 1.4219763279\tACC: 0.8494809866\n",
      "EP: 167\tLOSS: 1.4179451466\tACC: 0.8633217812\n",
      "EP: 168\tLOSS: 1.4273074865\tACC: 0.8442906737\n",
      "EP: 169\tLOSS: 1.4329737425\tACC: 0.8460207582\n",
      "EP: 170\tLOSS: 1.4280627966\tACC: 0.8460207582\n",
      "\tVALIDATION\tLOSS: 1.4023411274\tACC: 0.8726415038\n",
      "EP: 171\tLOSS: 1.4169806242\tACC: 0.8633217812\n",
      "EP: 172\tLOSS: 1.4092594385\tACC: 0.8667820096\n",
      "EP: 173\tLOSS: 1.4154653549\tACC: 0.8581314683\n",
      "EP: 174\tLOSS: 1.4157845974\tACC: 0.8546712995\n",
      "EP: 175\tLOSS: 1.4144563675\tACC: 0.8650519252\n",
      "EP: 176\tLOSS: 1.3911622763\tACC: 0.8875432611\n",
      "EP: 177\tLOSS: 1.4103302956\tACC: 0.8702422380\n",
      "EP: 178\tLOSS: 1.4154181480\tACC: 0.8564013839\n",
      "EP: 179\tLOSS: 1.4216790199\tACC: 0.8512110710\n",
      "EP: 180\tLOSS: 1.4266048670\tACC: 0.8442906737\n",
      "\tVALIDATION\tLOSS: 1.3757122755\tACC: 0.9009433985\n",
      "EP: 181\tLOSS: 1.4096963406\tACC: 0.8650519252\n",
      "EP: 182\tLOSS: 1.4216514826\tACC: 0.8494809866\n",
      "EP: 183\tLOSS: 1.4067628384\tACC: 0.8650519252\n",
      "EP: 184\tLOSS: 1.4297170639\tACC: 0.8477508426\n",
      "EP: 185\tLOSS: 1.3907954693\tACC: 0.8858131766\n",
      "EP: 186\tLOSS: 1.3955969810\tACC: 0.8788927197\n",
      "EP: 187\tLOSS: 1.4109289646\tACC: 0.8598616123\n",
      "EP: 188\tLOSS: 1.4048237801\tACC: 0.8702422380\n",
      "EP: 189\tLOSS: 1.4110060930\tACC: 0.8633217812\n",
      "EP: 190\tLOSS: 1.4057893753\tACC: 0.8719723225\n",
      "\tVALIDATION\tLOSS: 1.3827010393\tACC: 0.8867924809\n",
      "EP: 191\tLOSS: 1.4206296206\tACC: 0.8581314683\n",
      "EP: 192\tLOSS: 1.4092901945\tACC: 0.8650519252\n",
      "EP: 193\tLOSS: 1.3961420059\tACC: 0.8840830326\n",
      "EP: 194\tLOSS: 1.4175101519\tACC: 0.8512110710\n",
      "EP: 195\tLOSS: 1.3942312002\tACC: 0.8875432611\n",
      "EP: 196\tLOSS: 1.3846464157\tACC: 0.8927335739\n",
      "EP: 197\tLOSS: 1.3948587179\tACC: 0.8806228638\n",
      "EP: 198\tLOSS: 1.3845468760\tACC: 0.8892733455\n",
      "EP: 199\tLOSS: 1.3773578405\tACC: 0.8996539712\n",
      "EP: 200\tLOSS: 1.3816361427\tACC: 0.8927335739\n",
      "\tVALIDATION\tLOSS: 1.3873803616\tACC: 0.8820754886\n",
      "EP: 201\tLOSS: 1.3759269714\tACC: 0.8996539712\n",
      "EP: 202\tLOSS: 1.3822344542\tACC: 0.8979238868\n",
      "EP: 203\tLOSS: 1.3683129549\tACC: 0.9048442841\n",
      "EP: 204\tLOSS: 1.3922365904\tACC: 0.8788927197\n",
      "EP: 205\tLOSS: 1.3675482273\tACC: 0.9083045125\n",
      "EP: 206\tLOSS: 1.4067469835\tACC: 0.8650519252\n",
      "EP: 207\tLOSS: 1.3824689388\tACC: 0.8927335739\n",
      "EP: 208\tLOSS: 1.3842644691\tACC: 0.8875432611\n",
      "EP: 209\tLOSS: 1.3914579153\tACC: 0.8840830326\n",
      "EP: 210\tLOSS: 1.3828437328\tACC: 0.8927335739\n",
      "\tVALIDATION\tLOSS: 1.3650894165\tACC: 0.9103773832\n",
      "EP: 211\tLOSS: 1.3741197586\tACC: 0.8927335739\n",
      "EP: 212\tLOSS: 1.3798394203\tACC: 0.8927335739\n",
      "EP: 213\tLOSS: 1.3807890415\tACC: 0.8910034895\n",
      "EP: 214\tLOSS: 1.3764642477\tACC: 0.8996539712\n",
      "EP: 215\tLOSS: 1.3689056635\tACC: 0.9100345969\n",
      "EP: 216\tLOSS: 1.3754974604\tACC: 0.9031141996\n",
      "EP: 217\tLOSS: 1.3894406557\tACC: 0.8875432611\n",
      "EP: 218\tLOSS: 1.3836104870\tACC: 0.8892733455\n",
      "EP: 219\tLOSS: 1.3953539133\tACC: 0.8737024069\n",
      "EP: 220\tLOSS: 1.3775861263\tACC: 0.8961937428\n",
      "\tVALIDATION\tLOSS: 1.3451220989\tACC: 0.9245283008\n",
      "EP: 221\tLOSS: 1.3632578850\tACC: 0.9100345969\n",
      "EP: 222\tLOSS: 1.3631050587\tACC: 0.9117646813\n",
      "EP: 223\tLOSS: 1.3553137779\tACC: 0.9169549942\n",
      "EP: 224\tLOSS: 1.3817968369\tACC: 0.8875432611\n",
      "EP: 225\tLOSS: 1.3617788553\tACC: 0.9152249098\n",
      "EP: 226\tLOSS: 1.3533571959\tACC: 0.9238754511\n",
      "EP: 227\tLOSS: 1.3549485207\tACC: 0.9204152226\n",
      "EP: 228\tLOSS: 1.3623342514\tACC: 0.9117646813\n",
      "EP: 229\tLOSS: 1.3862770796\tACC: 0.8910034895\n",
      "EP: 230\tLOSS: 1.3616559505\tACC: 0.9134948254\n",
      "\tVALIDATION\tLOSS: 1.3313049078\tACC: 0.9433962107\n",
      "EP: 231\tLOSS: 1.3604114056\tACC: 0.9169549942\n",
      "EP: 232\tLOSS: 1.3612785339\tACC: 0.9169549942\n",
      "EP: 233\tLOSS: 1.3537956476\tACC: 0.9221453071\n",
      "EP: 234\tLOSS: 1.3627780676\tACC: 0.9100345969\n",
      "EP: 235\tLOSS: 1.3591334820\tACC: 0.9169549942\n",
      "EP: 236\tLOSS: 1.3780621290\tACC: 0.8996539712\n",
      "EP: 237\tLOSS: 1.3611829281\tACC: 0.9134948254\n",
      "EP: 238\tLOSS: 1.3554766178\tACC: 0.9186851382\n",
      "EP: 239\tLOSS: 1.3516330719\tACC: 0.9238754511\n",
      "EP: 240\tLOSS: 1.3584707975\tACC: 0.9134948254\n",
      "\tVALIDATION\tLOSS: 1.3318269253\tACC: 0.9433962107\n",
      "EP: 241\tLOSS: 1.3630406857\tACC: 0.9031141996\n",
      "EP: 242\tLOSS: 1.3413720131\tACC: 0.9325259328\n",
      "EP: 243\tLOSS: 1.3622524738\tACC: 0.9134948254\n",
      "EP: 244\tLOSS: 1.3566945791\tACC: 0.9169549942\n",
      "EP: 245\tLOSS: 1.3559398651\tACC: 0.9186851382\n",
      "EP: 246\tLOSS: 1.3498250246\tACC: 0.9238754511\n",
      "EP: 247\tLOSS: 1.3578420877\tACC: 0.9186851382\n",
      "EP: 248\tLOSS: 1.3590821028\tACC: 0.9152249098\n",
      "EP: 249\tLOSS: 1.3442311287\tACC: 0.9359861612\n",
      "EP: 250\tLOSS: 1.3466429710\tACC: 0.9307958484\n",
      "\tVALIDATION\tLOSS: 1.3502136469\tACC: 0.9198113084\n",
      "EP: 251\tLOSS: 1.3396577835\tACC: 0.9377162457\n",
      "EP: 252\tLOSS: 1.3489390612\tACC: 0.9273356199\n",
      "EP: 253\tLOSS: 1.3578153849\tACC: 0.9169549942\n",
      "EP: 254\tLOSS: 1.3383523226\tACC: 0.9359861612\n",
      "EP: 255\tLOSS: 1.3481086493\tACC: 0.9290657640\n",
      "EP: 256\tLOSS: 1.3645490408\tACC: 0.9065743685\n",
      "EP: 257\tLOSS: 1.3479794264\tACC: 0.9290657640\n",
      "EP: 258\tLOSS: 1.3474717140\tACC: 0.9307958484\n",
      "EP: 259\tLOSS: 1.3494836092\tACC: 0.9307958484\n",
      "EP: 260\tLOSS: 1.3549028635\tACC: 0.9238754511\n",
      "\tVALIDATION\tLOSS: 1.3344298601\tACC: 0.9386792183\n",
      "EP: 261\tLOSS: 1.3484708071\tACC: 0.9307958484\n",
      "EP: 262\tLOSS: 1.3404982090\tACC: 0.9377162457\n",
      "EP: 263\tLOSS: 1.3388773203\tACC: 0.9377162457\n",
      "EP: 264\tLOSS: 1.3424427509\tACC: 0.9325259328\n",
      "EP: 265\tLOSS: 1.3335127831\tACC: 0.9446367025\n",
      "EP: 266\tLOSS: 1.3438528776\tACC: 0.9307958484\n",
      "EP: 267\tLOSS: 1.3396288157\tACC: 0.9377162457\n",
      "EP: 268\tLOSS: 1.3432962894\tACC: 0.9290657640\n",
      "EP: 269\tLOSS: 1.3331164122\tACC: 0.9411764741\n",
      "EP: 270\tLOSS: 1.3418242931\tACC: 0.9342560768\n",
      "\tVALIDATION\tLOSS: 1.3453644514\tACC: 0.9292452931\n",
      "EP: 271\tLOSS: 1.3412399292\tACC: 0.9307958484\n",
      "EP: 272\tLOSS: 1.3353556395\tACC: 0.9394463897\n",
      "EP: 273\tLOSS: 1.3479256630\tACC: 0.9290657640\n",
      "EP: 274\tLOSS: 1.3294799328\tACC: 0.9498270154\n",
      "EP: 275\tLOSS: 1.3260133266\tACC: 0.9498270154\n",
      "EP: 276\tLOSS: 1.3314509392\tACC: 0.9429065585\n",
      "EP: 277\tLOSS: 1.3366488218\tACC: 0.9429065585\n",
      "EP: 278\tLOSS: 1.3323222399\tACC: 0.9394463897\n",
      "EP: 279\tLOSS: 1.3213515282\tACC: 0.9567474127\n",
      "EP: 280\tLOSS: 1.3443912268\tACC: 0.9290657640\n",
      "\tVALIDATION\tLOSS: 1.3351509571\tACC: 0.9386792183\n",
      "EP: 281\tLOSS: 1.3277837038\tACC: 0.9498270154\n",
      "EP: 282\tLOSS: 1.3362697363\tACC: 0.9359861612\n",
      "EP: 283\tLOSS: 1.3350291252\tACC: 0.9394463897\n",
      "EP: 284\tLOSS: 1.3273843527\tACC: 0.9463667870\n",
      "EP: 285\tLOSS: 1.3310240507\tACC: 0.9480968714\n",
      "EP: 286\tLOSS: 1.3271672726\tACC: 0.9498270154\n",
      "EP: 287\tLOSS: 1.3195908070\tACC: 0.9532871842\n",
      "EP: 288\tLOSS: 1.3239850998\tACC: 0.9498270154\n",
      "EP: 289\tLOSS: 1.3275028467\tACC: 0.9446367025\n",
      "EP: 290\tLOSS: 1.3193657398\tACC: 0.9550173283\n",
      "\tVALIDATION\tLOSS: 1.3367363214\tACC: 0.9386792183\n",
      "EP: 291\tLOSS: 1.3379191160\tACC: 0.9325259328\n",
      "EP: 292\tLOSS: 1.3225361109\tACC: 0.9532871842\n",
      "EP: 293\tLOSS: 1.3190132380\tACC: 0.9550173283\n",
      "EP: 294\tLOSS: 1.3272418976\tACC: 0.9480968714\n",
      "EP: 295\tLOSS: 1.3255337477\tACC: 0.9480968714\n",
      "EP: 296\tLOSS: 1.3181111813\tACC: 0.9567474127\n",
      "EP: 297\tLOSS: 1.3381018639\tACC: 0.9359861612\n",
      "EP: 298\tLOSS: 1.3264063597\tACC: 0.9446367025\n",
      "EP: 299\tLOSS: 1.3318668604\tACC: 0.9429065585\n",
      "EP: 300\tLOSS: 1.3320754766\tACC: 0.9446367025\n",
      "\tVALIDATION\tLOSS: 1.3391112089\tACC: 0.9339622855\n",
      "EP: 301\tLOSS: 1.3241230249\tACC: 0.9480968714\n",
      "EP: 302\tLOSS: 1.3214647770\tACC: 0.9515570998\n",
      "EP: 303\tLOSS: 1.3349447250\tACC: 0.9446367025\n",
      "EP: 304\tLOSS: 1.3209149837\tACC: 0.9550173283\n",
      "EP: 305\tLOSS: 1.3197941780\tACC: 0.9567474127\n",
      "EP: 306\tLOSS: 1.3164623976\tACC: 0.9584774971\n",
      "EP: 307\tLOSS: 1.3259340525\tACC: 0.9515570998\n",
      "EP: 308\tLOSS: 1.3374500275\tACC: 0.9377162457\n",
      "EP: 309\tLOSS: 1.3305833340\tACC: 0.9446367025\n",
      "EP: 310\tLOSS: 1.3272684813\tACC: 0.9480968714\n",
      "\tVALIDATION\tLOSS: 1.3280866146\tACC: 0.9481132030\n",
      "EP: 311\tLOSS: 1.3169775009\tACC: 0.9567474127\n",
      "EP: 312\tLOSS: 1.3163607121\tACC: 0.9619377255\n",
      "EP: 313\tLOSS: 1.3197667599\tACC: 0.9550173283\n",
      "EP: 314\tLOSS: 1.3235386610\tACC: 0.9532871842\n",
      "EP: 315\tLOSS: 1.3190066814\tACC: 0.9567474127\n",
      "EP: 316\tLOSS: 1.3136129379\tACC: 0.9636678100\n",
      "EP: 317\tLOSS: 1.3208253384\tACC: 0.9532871842\n",
      "EP: 318\tLOSS: 1.3250311613\tACC: 0.9498270154\n",
      "EP: 319\tLOSS: 1.3217499256\tACC: 0.9602076411\n",
      "EP: 320\tLOSS: 1.3238360882\tACC: 0.9498270154\n",
      "\tVALIDATION\tLOSS: 1.3398973942\tACC: 0.9339622855\n",
      "EP: 321\tLOSS: 1.3166074753\tACC: 0.9602076411\n",
      "EP: 322\tLOSS: 1.3269519806\tACC: 0.9498270154\n",
      "EP: 323\tLOSS: 1.3244796991\tACC: 0.9498270154\n",
      "EP: 324\tLOSS: 1.3197764158\tACC: 0.9532871842\n",
      "EP: 325\tLOSS: 1.3207941055\tACC: 0.9550173283\n",
      "EP: 326\tLOSS: 1.3214310408\tACC: 0.9515570998\n",
      "EP: 327\tLOSS: 1.3288046122\tACC: 0.9480968714\n",
      "EP: 328\tLOSS: 1.3131304979\tACC: 0.9636678100\n",
      "EP: 329\tLOSS: 1.3210574389\tACC: 0.9567474127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP: 330\tLOSS: 1.3208101988\tACC: 0.9515570998\n",
      "\tVALIDATION\tLOSS: 1.3215858936\tACC: 0.9528301954\n",
      "EP: 331\tLOSS: 1.3094278574\tACC: 0.9688581228\n",
      "EP: 332\tLOSS: 1.3151898384\tACC: 0.9619377255\n",
      "EP: 333\tLOSS: 1.3172676563\tACC: 0.9619377255\n",
      "EP: 334\tLOSS: 1.3131403923\tACC: 0.9653978944\n",
      "EP: 335\tLOSS: 1.3189922571\tACC: 0.9532871842\n",
      "EP: 336\tLOSS: 1.3248375654\tACC: 0.9480968714\n",
      "EP: 337\tLOSS: 1.3140869141\tACC: 0.9619377255\n",
      "EP: 338\tLOSS: 1.2970807552\tACC: 0.9809688330\n",
      "EP: 339\tLOSS: 1.3098562956\tACC: 0.9671280384\n",
      "EP: 340\tLOSS: 1.3134199381\tACC: 0.9619377255\n",
      "\tVALIDATION\tLOSS: 1.3262406588\tACC: 0.9481132030\n",
      "EP: 341\tLOSS: 1.3114860058\tACC: 0.9636678100\n",
      "EP: 342\tLOSS: 1.3137609959\tACC: 0.9619377255\n",
      "EP: 343\tLOSS: 1.3224085569\tACC: 0.9515570998\n",
      "EP: 344\tLOSS: 1.3137152195\tACC: 0.9602076411\n",
      "EP: 345\tLOSS: 1.3158627748\tACC: 0.9619377255\n",
      "EP: 346\tLOSS: 1.3178564310\tACC: 0.9567474127\n",
      "EP: 347\tLOSS: 1.3134284019\tACC: 0.9602076411\n",
      "EP: 348\tLOSS: 1.3122661114\tACC: 0.9653978944\n",
      "EP: 349\tLOSS: 1.3130257130\tACC: 0.9619377255\n",
      "EP: 350\tLOSS: 1.3161021471\tACC: 0.9567474127\n",
      "\tVALIDATION\tLOSS: 1.3240835667\tACC: 0.9528301954\n",
      "EP: 351\tLOSS: 1.3161413670\tACC: 0.9619377255\n",
      "EP: 352\tLOSS: 1.3130220175\tACC: 0.9602076411\n",
      "EP: 353\tLOSS: 1.3197057247\tACC: 0.9515570998\n",
      "EP: 354\tLOSS: 1.3178323507\tACC: 0.9584774971\n",
      "EP: 355\tLOSS: 1.3101376295\tACC: 0.9671280384\n",
      "EP: 356\tLOSS: 1.3163299561\tACC: 0.9584774971\n",
      "EP: 357\tLOSS: 1.3122586012\tACC: 0.9636678100\n",
      "EP: 358\tLOSS: 1.3289438486\tACC: 0.9463667870\n",
      "EP: 359\tLOSS: 1.3136887550\tACC: 0.9584774971\n",
      "EP: 360\tLOSS: 1.3072252274\tACC: 0.9671280384\n",
      "\tVALIDATION\tLOSS: 1.3155761957\tACC: 0.9575471878\n",
      "EP: 361\tLOSS: 1.3112580776\tACC: 0.9636678100\n",
      "EP: 362\tLOSS: 1.3194110394\tACC: 0.9532871842\n",
      "EP: 363\tLOSS: 1.3183289766\tACC: 0.9584774971\n",
      "EP: 364\tLOSS: 1.3138742447\tACC: 0.9602076411\n",
      "EP: 365\tLOSS: 1.3125438690\tACC: 0.9653978944\n",
      "EP: 366\tLOSS: 1.3088192940\tACC: 0.9636678100\n",
      "EP: 367\tLOSS: 1.3164396286\tACC: 0.9567474127\n",
      "EP: 368\tLOSS: 1.3139039278\tACC: 0.9619377255\n",
      "EP: 369\tLOSS: 1.3064383268\tACC: 0.9688581228\n",
      "EP: 370\tLOSS: 1.2985048294\tACC: 0.9757785201\n",
      "\tVALIDATION\tLOSS: 1.3111059666\tACC: 0.9622641802\n",
      "EP: 371\tLOSS: 1.3103494644\tACC: 0.9636678100\n",
      "EP: 372\tLOSS: 1.3111910820\tACC: 0.9636678100\n",
      "EP: 373\tLOSS: 1.3101460934\tACC: 0.9636678100\n",
      "EP: 374\tLOSS: 1.3048896790\tACC: 0.9705882072\n",
      "EP: 375\tLOSS: 1.3042240143\tACC: 0.9688581228\n",
      "EP: 376\tLOSS: 1.3061430454\tACC: 0.9723183513\n",
      "EP: 377\tLOSS: 1.3010115623\tACC: 0.9740484357\n",
      "EP: 378\tLOSS: 1.3060410023\tACC: 0.9653978944\n",
      "EP: 379\tLOSS: 1.3095247746\tACC: 0.9636678100\n",
      "EP: 380\tLOSS: 1.3067817688\tACC: 0.9688581228\n",
      "\tVALIDATION\tLOSS: 1.3079332113\tACC: 0.9669811130\n",
      "EP: 381\tLOSS: 1.3009244204\tACC: 0.9757785201\n",
      "EP: 382\tLOSS: 1.3100000620\tACC: 0.9636678100\n",
      "EP: 383\tLOSS: 1.3014485836\tACC: 0.9757785201\n",
      "EP: 384\tLOSS: 1.3086205721\tACC: 0.9671280384\n",
      "EP: 385\tLOSS: 1.3150830269\tACC: 0.9550173283\n",
      "EP: 386\tLOSS: 1.3223454952\tACC: 0.9515570998\n",
      "EP: 387\tLOSS: 1.3028463125\tACC: 0.9740484357\n",
      "EP: 388\tLOSS: 1.3019939661\tACC: 0.9705882072\n",
      "EP: 389\tLOSS: 1.3048828840\tACC: 0.9723183513\n",
      "EP: 390\tLOSS: 1.3060352802\tACC: 0.9671280384\n",
      "\tVALIDATION\tLOSS: 1.3020948172\tACC: 0.9716981053\n",
      "EP: 391\tLOSS: 1.3101943731\tACC: 0.9653978944\n",
      "EP: 392\tLOSS: 1.3136916161\tACC: 0.9636678100\n",
      "EP: 393\tLOSS: 1.2963684797\tACC: 0.9809688330\n",
      "EP: 394\tLOSS: 1.3065458536\tACC: 0.9723183513\n",
      "EP: 395\tLOSS: 1.3012478352\tACC: 0.9723183513\n",
      "EP: 396\tLOSS: 1.3109925985\tACC: 0.9636678100\n",
      "EP: 397\tLOSS: 1.3100558519\tACC: 0.9653978944\n",
      "EP: 398\tLOSS: 1.3110985756\tACC: 0.9619377255\n",
      "EP: 399\tLOSS: 1.3200637102\tACC: 0.9550173283\n"
     ]
    }
   ],
   "source": [
    "m.train(dat['train'], dat['test'], 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T21:35:23.354135Z",
     "start_time": "2019-02-06T21:35:23.224180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVALIDATION\tLOSS: 1.3120685816\tACC: 0.9622641802\n"
     ]
    }
   ],
   "source": [
    "m.test(dat['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
