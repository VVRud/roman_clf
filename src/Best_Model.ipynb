{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T18:10:07.290223Z",
     "start_time": "2019-02-06T18:10:05.783874Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "sys.path.extend(['..'])\n",
    "\n",
    "from utils.config import process_config\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.layers import (conv2d, max_pooling2d, average_pooling2d, batch_normalization, dropout, dense)\n",
    "from tensorflow.nn import (relu, sigmoid, softmax, leaky_relu)\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T18:10:07.312740Z",
     "start_time": "2019-02-06T18:10:07.310087Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA = '../data/data_clean/'\n",
    "CONF = '../configs/roman.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T18:10:07.334418Z",
     "start_time": "2019-02-06T18:10:07.329977Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    return (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "def shuffle_sim(a, b):\n",
    "    assert a.shape[0] == a.shape[0], 'Shapes must be equal'\n",
    "    \n",
    "    ind = np.arange(a.shape[0])\n",
    "    np.random.shuffle(ind)\n",
    "    return a[ind], b[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T18:10:07.358300Z",
     "start_time": "2019-02-06T18:10:07.349751Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_train_test(path_to_data):\n",
    "    data = {}\n",
    "    for dset in ['train', 'test']:\n",
    "        path_ = os.path.join(path_to_data, dset)\n",
    "        X, Y = [], []\n",
    "        classes = [d for d in os.listdir(path_) if os.path.isdir(os.path.join(path_, d))]\n",
    "        classes.sort()\n",
    "        \n",
    "        for cl in classes:\n",
    "            y = np.zeros((1, 8), dtype=np.int32)\n",
    "            y[0, int(cl) - 1] = 1\n",
    "            \n",
    "            cl_path = os.path.join(path_, cl)\n",
    "            filenames = [os.path.join(cl_path, pict) for pict in os.listdir(cl_path) if pict.endswith('.jpg')]\n",
    "            \n",
    "            for im in filenames:\n",
    "                image = np.asarray(Image.open(im), dtype=np.float32)\n",
    "                X.append(normalize(image).reshape((1, image.shape[0], image.shape[1], image.shape[2])))\n",
    "                Y.append(y)\n",
    "        \n",
    "        a, b = shuffle_sim(np.concatenate(X), np.concatenate(Y))\n",
    "        data[dset] = ([a, b])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T18:10:49.817144Z",
     "start_time": "2019-02-06T18:10:49.785993Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self, config, sess_cf, learning_rate):\n",
    "        self.lr = learning_rate\n",
    "        self.sess = tf.Session(config=sess_cf)\n",
    "\n",
    "        self.x = tf.placeholder(dtype=tf.float32, shape=(None, config.image_size, config.image_size, 3))\n",
    "        self.y = tf.placeholder(dtype=tf.int32, shape=(None, 8))\n",
    "        self.training = tf.placeholder(dtype=tf.bool, shape=())\n",
    "\n",
    "        global_step = tf.Variable(1, name='global_step', trainable=False, dtype=tf.int32)\n",
    "        self.step = tf.assign(global_step, global_step + 1)\n",
    "        \n",
    "        self.model()\n",
    "        \n",
    "        self.summ_writer = tf.summary.FileWriter(config.summary_dir, graph=self.sess.graph)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def block(self, inp,\n",
    "              ch,\n",
    "              num,\n",
    "              c_ker=[(3, 3), (3, 3)],\n",
    "              c_str=[(1, 1), (1, 1)],\n",
    "              act=relu,\n",
    "              mp_ker=(2, 2),\n",
    "              mp_str=(2, 2),\n",
    "              mode='conc'):\n",
    "    \n",
    "        with tf.variable_scope('block_' + str(num), reuse=tf.AUTO_REUSE):\n",
    "            conv = conv2d(inp, ch, c_ker[0], strides=c_str[0])\n",
    "            bn = batch_normalization(conv)\n",
    "            out = act(bn)\n",
    "            out = dropout(out, 0.2)\n",
    "            tf.summary.histogram('conv1', conv)\n",
    "            print(out.shape)\n",
    "            \n",
    "            conv = conv2d(out, ch, c_ker[1], strides=c_str[1])\n",
    "            bn = batch_normalization(conv)\n",
    "            out = act(bn)\n",
    "            tf.summary.histogram('conv2', conv)\n",
    "            print(out.shape)\n",
    "            \n",
    "            mp = max_pooling2d(out, mp_ker, strides=mp_str)\n",
    "            if mode == 'mp':\n",
    "                out = mp\n",
    "            elif mode == 'conc':\n",
    "                out = tf.concat([mp, average_pooling2d(out, mp_ker, mp_str)], -1)\n",
    "            else:\n",
    "                raise ValueError('Unknown mode.')\n",
    "            \n",
    "            print(out.shape)\n",
    "        return out\n",
    "    \n",
    "    def model(self):\n",
    "        with tf.name_scope('layers'):\n",
    "            out = self.block(self.x, 16, 1, mode='conc')\n",
    "            out = self.block(out, 32, 2, mode='conc')\n",
    "            out = self.block(out, 64, 3, mode='conc')\n",
    "            out = self.block(out, 256, 4, c_str=[(1, 1), (2, 2)], mode='mp')\n",
    "            \n",
    "            dim = np.prod(out.shape[1:])\n",
    "            out = tf.reshape(out, [-1, dim])\n",
    "            print(out.shape)\n",
    "            \n",
    "            dense_l = dense(out, 128)\n",
    "            tf.summary.histogram('dense2', dense_l)\n",
    "            out = batch_normalization(dense_l)\n",
    "            out = leaky_relu(out, alpha=0.01)\n",
    "            out = dropout(out, rate=0.7, training=self.training)\n",
    "            print(out.shape)\n",
    "\n",
    "            self.predictions = dense(out, 8, activation=softmax)\n",
    "            tf.summary.histogram('pred', self.predictions)\n",
    "\n",
    "        with tf.name_scope('metrics'):    \n",
    "            amax_labels = tf.argmax(self.y, 1)\n",
    "            amax_pred   = tf.argmax(self.predictions, 1)\n",
    "\n",
    "            self.loss = tf.losses.softmax_cross_entropy(self.y, self.predictions)        \n",
    "            self.acc = tf.reduce_mean(tf.cast(tf.equal(amax_labels, amax_pred), dtype=tf.float32))\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            tf.summary.scalar('accuracy', self.acc)\n",
    "\n",
    "        self.summary = tf.summary.merge_all()\n",
    "        \n",
    "    def train(self, dat, dat_v, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            loss, acc, _, summary, step = self.sess.run([\n",
    "                self.loss, self.acc, self.optimizer, self.summary, self.step\n",
    "            ],\n",
    "                                                feed_dict={\n",
    "                                                    self.x: dat[0],\n",
    "                                                    self.y: dat[1],\n",
    "                                                    self.training: True\n",
    "                                                })\n",
    "\n",
    "            self.summ_writer.add_summary(summary, step)\n",
    "            print('EP: {:3d}\\tLOSS: {:.10f}\\tACC: {:.10f}'.format(\n",
    "                epoch, loss, acc))\n",
    "\n",
    "            if epoch % 10 == 0 and epoch != 0:\n",
    "                self.test(dat_v)\n",
    "                \n",
    "    def test(self, dat):\n",
    "        loss, acc = self.sess.run([self.loss, self.acc],\n",
    "                                         feed_dict={self.x: dat[0],\n",
    "                                                    self.y: dat[1],\n",
    "                                                    self.training: False})\n",
    "\n",
    "        print('\\tVALIDATION\\tLOSS: {:.10f}\\tACC: {:.10f}'.format(loss, acc))\n",
    "    \n",
    "    def close(self):\n",
    "        self.sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T18:10:51.511900Z",
     "start_time": "2019-02-06T18:10:51.497555Z"
    }
   },
   "outputs": [],
   "source": [
    "m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T18:10:52.652695Z",
     "start_time": "2019-02-06T18:10:52.648900Z"
    }
   },
   "outputs": [],
   "source": [
    "config_tf = tf.ConfigProto(allow_soft_placement=True)\n",
    "config_tf.gpu_options.allow_growth = True\n",
    "config_tf.gpu_options.per_process_gpu_memory_fraction = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T18:10:52.891926Z",
     "start_time": "2019-02-06T18:10:52.887948Z"
    }
   },
   "outputs": [],
   "source": [
    "config = process_config(CONF)\n",
    "config['exp_name'] = '4b_ap_mp'\n",
    "config['summary_dir'] = '../experiments/' + config['exp_name'] + '/summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T18:10:55.604202Z",
     "start_time": "2019-02-06T18:10:53.725214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 126, 126, 16)\n",
      "(?, 124, 124, 16)\n",
      "(?, 62, 62, 32)\n",
      "(?, 60, 60, 32)\n",
      "(?, 58, 58, 32)\n",
      "(?, 29, 29, 64)\n",
      "(?, 27, 27, 64)\n",
      "(?, 25, 25, 64)\n",
      "(?, 12, 12, 128)\n",
      "(?, 10, 10, 256)\n",
      "(?, 4, 4, 256)\n",
      "(?, 2, 2, 256)\n",
      "(?, 1024)\n",
      "(?, 128)\n"
     ]
    }
   ],
   "source": [
    "m = Model(config, config_tf, 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T18:10:59.444447Z",
     "start_time": "2019-02-06T18:10:58.903753Z"
    }
   },
   "outputs": [],
   "source": [
    "dat = read_train_test(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T18:31:20.138032Z",
     "start_time": "2019-02-06T18:10:59.519812Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP:   0\tLOSS: 2.0795323849\tACC: 0.1475054175\n",
      "EP:   1\tLOSS: 2.0796723366\tACC: 0.1062906757\n",
      "EP:   2\tLOSS: 2.0793409348\tACC: 0.1279826462\n",
      "EP:   3\tLOSS: 2.0787518024\tACC: 0.1323210448\n",
      "EP:   4\tLOSS: 2.0788757801\tACC: 0.1431670338\n",
      "EP:   5\tLOSS: 2.0786354542\tACC: 0.1409978271\n",
      "EP:   6\tLOSS: 2.0809714794\tACC: 0.1214750558\n",
      "EP:   7\tLOSS: 2.0769274235\tACC: 0.1431670338\n",
      "EP:   8\tLOSS: 2.0793395042\tACC: 0.1301518381\n",
      "EP:   9\tLOSS: 2.0775713921\tACC: 0.1301518381\n",
      "EP:  10\tLOSS: 2.0770213604\tACC: 0.1409978271\n",
      "\tVALIDATION\tLOSS: 2.0753703117\tACC: 0.1416666657\n",
      "EP:  11\tLOSS: 2.0734472275\tACC: 0.1778741926\n",
      "EP:  12\tLOSS: 2.0722839832\tACC: 0.1431670338\n",
      "EP:  13\tLOSS: 2.0711917877\tACC: 0.1800433844\n",
      "EP:  14\tLOSS: 2.0759205818\tACC: 0.1301518381\n",
      "EP:  15\tLOSS: 2.0657722950\tACC: 0.1605206132\n",
      "EP:  16\tLOSS: 2.0559904575\tACC: 0.1952277720\n",
      "EP:  17\tLOSS: 2.0527453423\tACC: 0.1930585653\n",
      "EP:  18\tLOSS: 2.0460200310\tACC: 0.1843817830\n",
      "EP:  19\tLOSS: 2.0345833302\tACC: 0.2255965322\n",
      "EP:  20\tLOSS: 2.0139260292\tACC: 0.2299349308\n",
      "\tVALIDATION\tLOSS: 1.9805331230\tACC: 0.2333333343\n",
      "EP:  21\tLOSS: 2.0011253357\tACC: 0.2603036761\n",
      "EP:  22\tLOSS: 1.9935501814\tACC: 0.2472885102\n",
      "EP:  23\tLOSS: 1.9785859585\tACC: 0.2841648459\n",
      "EP:  24\tLOSS: 1.9472767115\tACC: 0.3101952374\n",
      "EP:  25\tLOSS: 1.9541639090\tACC: 0.3167028129\n",
      "EP:  26\tLOSS: 1.9491394758\tACC: 0.3362255991\n",
      "EP:  27\tLOSS: 1.9424450397\tACC: 0.3492407799\n",
      "EP:  28\tLOSS: 1.9215756655\tACC: 0.3427331746\n",
      "EP:  29\tLOSS: 1.9147088528\tACC: 0.3557483852\n",
      "EP:  30\tLOSS: 1.9247257710\tACC: 0.3297179937\n",
      "\tVALIDATION\tLOSS: 1.8543945551\tACC: 0.4333333373\n",
      "EP:  31\tLOSS: 1.9054597616\tACC: 0.3449023962\n",
      "EP:  32\tLOSS: 1.8680235147\tACC: 0.3904555440\n",
      "EP:  33\tLOSS: 1.8743817806\tACC: 0.3969631195\n",
      "EP:  34\tLOSS: 1.8641194105\tACC: 0.4121474922\n",
      "EP:  35\tLOSS: 1.8622251749\tACC: 0.4013015330\n",
      "EP:  36\tLOSS: 1.8547805548\tACC: 0.4078091085\n",
      "EP:  37\tLOSS: 1.8438181877\tACC: 0.4164859056\n",
      "EP:  38\tLOSS: 1.8121961355\tACC: 0.4577006400\n",
      "EP:  39\tLOSS: 1.8370095491\tACC: 0.4381778836\n",
      "EP:  40\tLOSS: 1.8277885914\tACC: 0.4360086620\n",
      "\tVALIDATION\tLOSS: 1.8170608282\tACC: 0.4583333433\n",
      "EP:  41\tLOSS: 1.8148419857\tACC: 0.4468546510\n",
      "EP:  42\tLOSS: 1.7996346951\tACC: 0.4815618098\n",
      "EP:  43\tLOSS: 1.7847286463\tACC: 0.4815618098\n",
      "EP:  44\tLOSS: 1.7974334955\tACC: 0.4793926179\n",
      "EP:  45\tLOSS: 1.7808970213\tACC: 0.4880694151\n",
      "EP:  46\tLOSS: 1.7997255325\tACC: 0.4663774371\n",
      "EP:  47\tLOSS: 1.7689135075\tACC: 0.5032538176\n",
      "EP:  48\tLOSS: 1.7764317989\tACC: 0.4945770204\n",
      "EP:  49\tLOSS: 1.7671257257\tACC: 0.5032538176\n",
      "EP:  50\tLOSS: 1.7792433500\tACC: 0.4902386069\n",
      "\tVALIDATION\tLOSS: 1.7576612234\tACC: 0.5166666508\n",
      "EP:  51\tLOSS: 1.7675675154\tACC: 0.5097613931\n",
      "EP:  52\tLOSS: 1.7213028669\tACC: 0.5574837327\n",
      "EP:  53\tLOSS: 1.7533531189\tACC: 0.5184381604\n",
      "EP:  54\tLOSS: 1.7682710886\tACC: 0.5075922012\n",
      "EP:  55\tLOSS: 1.7571403980\tACC: 0.5162689686\n",
      "EP:  56\tLOSS: 1.7342910767\tACC: 0.5357917547\n",
      "EP:  57\tLOSS: 1.7384426594\tACC: 0.5292841792\n",
      "EP:  58\tLOSS: 1.7470512390\tACC: 0.5097613931\n",
      "EP:  59\tLOSS: 1.7180328369\tACC: 0.5574837327\n",
      "EP:  60\tLOSS: 1.7311642170\tACC: 0.5314533710\n",
      "\tVALIDATION\tLOSS: 1.7531298399\tACC: 0.5166666508\n",
      "EP:  61\tLOSS: 1.7296710014\tACC: 0.5444685221\n",
      "EP:  62\tLOSS: 1.7031143904\tACC: 0.5791757107\n",
      "EP:  63\tLOSS: 1.7172598839\tACC: 0.5639913082\n",
      "EP:  64\tLOSS: 1.7296839952\tACC: 0.5422993302\n",
      "EP:  65\tLOSS: 1.6892932653\tACC: 0.5856832862\n",
      "EP:  66\tLOSS: 1.7278796434\tACC: 0.5509761572\n",
      "EP:  67\tLOSS: 1.6769938469\tACC: 0.5921908617\n",
      "EP:  68\tLOSS: 1.6915260553\tACC: 0.5856832862\n",
      "EP:  69\tLOSS: 1.7096952200\tACC: 0.5596529245\n",
      "EP:  70\tLOSS: 1.6917127371\tACC: 0.5835140944\n",
      "\tVALIDATION\tLOSS: 1.7306047678\tACC: 0.5416666865\n",
      "EP:  71\tLOSS: 1.7048174143\tACC: 0.5661605000\n",
      "EP:  72\tLOSS: 1.6902616024\tACC: 0.5856832862\n",
      "EP:  73\tLOSS: 1.6782848835\tACC: 0.5921908617\n",
      "EP:  74\tLOSS: 1.6834522486\tACC: 0.5921908617\n",
      "EP:  75\tLOSS: 1.6684249640\tACC: 0.6008676887\n",
      "EP:  76\tLOSS: 1.6551967859\tACC: 0.6138828397\n",
      "EP:  77\tLOSS: 1.6889698505\tACC: 0.5813449025\n",
      "EP:  78\tLOSS: 1.6361714602\tACC: 0.6399132609\n",
      "EP:  79\tLOSS: 1.6726697683\tACC: 0.5965293050\n",
      "EP:  80\tLOSS: 1.6459455490\tACC: 0.6312364340\n",
      "\tVALIDATION\tLOSS: 1.6558558941\tACC: 0.6250000000\n",
      "EP:  81\tLOSS: 1.6422251463\tACC: 0.6377440095\n",
      "EP:  82\tLOSS: 1.6592172384\tACC: 0.6073752642\n",
      "EP:  83\tLOSS: 1.6611864567\tACC: 0.6073752642\n",
      "EP:  84\tLOSS: 1.6193912029\tACC: 0.6550976038\n",
      "EP:  85\tLOSS: 1.6444376707\tACC: 0.6312364340\n",
      "EP:  86\tLOSS: 1.6576294899\tACC: 0.6203904748\n",
      "EP:  87\tLOSS: 1.6462996006\tACC: 0.6225596666\n",
      "EP:  88\tLOSS: 1.6375464201\tACC: 0.6377440095\n",
      "EP:  89\tLOSS: 1.6462765932\tACC: 0.6138828397\n",
      "EP:  90\tLOSS: 1.5914676189\tACC: 0.6832971573\n",
      "\tVALIDATION\tLOSS: 1.6590132713\tACC: 0.6083333492\n",
      "EP:  91\tLOSS: 1.6359084845\tACC: 0.6377440095\n",
      "EP:  92\tLOSS: 1.6205563545\tACC: 0.6572667956\n",
      "EP:  93\tLOSS: 1.5903998613\tACC: 0.6941431761\n",
      "EP:  94\tLOSS: 1.5893040895\tACC: 0.6789587736\n",
      "EP:  95\tLOSS: 1.5964815617\tACC: 0.6767895818\n",
      "EP:  96\tLOSS: 1.6025620699\tACC: 0.6832971573\n",
      "EP:  97\tLOSS: 1.6135288477\tACC: 0.6550976038\n",
      "EP:  98\tLOSS: 1.6123036146\tACC: 0.6637744308\n",
      "EP:  99\tLOSS: 1.5907055140\tACC: 0.6832971573\n",
      "EP: 100\tLOSS: 1.5676761866\tACC: 0.7049891353\n",
      "\tVALIDATION\tLOSS: 1.6249419451\tACC: 0.6499999762\n",
      "EP: 101\tLOSS: 1.5575217009\tACC: 0.7266811132\n",
      "EP: 102\tLOSS: 1.5793399811\tACC: 0.7028199434\n",
      "EP: 103\tLOSS: 1.5588529110\tACC: 0.7158351541\n",
      "EP: 104\tLOSS: 1.5674200058\tACC: 0.7180043459\n",
      "EP: 105\tLOSS: 1.5811468363\tACC: 0.7006507516\n",
      "EP: 106\tLOSS: 1.5764696598\tACC: 0.6898047924\n",
      "EP: 107\tLOSS: 1.5769655704\tACC: 0.6941431761\n",
      "EP: 108\tLOSS: 1.5371732712\tACC: 0.7505422831\n",
      "EP: 109\tLOSS: 1.5451891422\tACC: 0.7396963239\n",
      "EP: 110\tLOSS: 1.5500373840\tACC: 0.7223427296\n",
      "\tVALIDATION\tLOSS: 1.6039801836\tACC: 0.6750000119\n",
      "EP: 111\tLOSS: 1.5329281092\tACC: 0.7483730912\n",
      "EP: 112\tLOSS: 1.5391812325\tACC: 0.7375271320\n",
      "EP: 113\tLOSS: 1.5308268070\tACC: 0.7440347075\n",
      "EP: 114\tLOSS: 1.5289529562\tACC: 0.7462038994\n",
      "EP: 115\tLOSS: 1.5258162022\tACC: 0.7440347075\n",
      "EP: 116\tLOSS: 1.5091084242\tACC: 0.7592191100\n",
      "EP: 117\tLOSS: 1.5223468542\tACC: 0.7440347075\n",
      "EP: 118\tLOSS: 1.5485553741\tACC: 0.7245119214\n",
      "EP: 119\tLOSS: 1.5045520067\tACC: 0.7700650692\n",
      "EP: 120\tLOSS: 1.5182894468\tACC: 0.7505422831\n",
      "\tVALIDATION\tLOSS: 1.5853462219\tACC: 0.6666666865\n",
      "EP: 121\tLOSS: 1.5079888105\tACC: 0.7678958774\n",
      "EP: 122\tLOSS: 1.5150307417\tACC: 0.7678958774\n",
      "EP: 123\tLOSS: 1.5050029755\tACC: 0.7744034529\n",
      "EP: 124\tLOSS: 1.5025488138\tACC: 0.7830802798\n",
      "EP: 125\tLOSS: 1.5171661377\tACC: 0.7548806667\n",
      "EP: 126\tLOSS: 1.4941902161\tACC: 0.7830802798\n",
      "EP: 127\tLOSS: 1.5112876892\tACC: 0.7635574937\n",
      "EP: 128\tLOSS: 1.4912432432\tACC: 0.7939262390\n",
      "EP: 129\tLOSS: 1.5086394548\tACC: 0.7570499182\n",
      "EP: 130\tLOSS: 1.5125515461\tACC: 0.7657266855\n",
      "\tVALIDATION\tLOSS: 1.5785871744\tACC: 0.6916666627\n",
      "EP: 131\tLOSS: 1.4966466427\tACC: 0.7830802798\n",
      "EP: 132\tLOSS: 1.4553745985\tACC: 0.8242949843\n",
      "EP: 133\tLOSS: 1.4860137701\tACC: 0.7874186635\n",
      "EP: 134\tLOSS: 1.5119098425\tACC: 0.7700650692\n",
      "EP: 135\tLOSS: 1.5090053082\tACC: 0.7700650692\n",
      "EP: 136\tLOSS: 1.4834332466\tACC: 0.7939262390\n",
      "EP: 137\tLOSS: 1.4702885151\tACC: 0.8004338145\n",
      "EP: 138\tLOSS: 1.4898059368\tACC: 0.7895878553\n",
      "EP: 139\tLOSS: 1.4839743376\tACC: 0.7939262390\n",
      "EP: 140\tLOSS: 1.4704747200\tACC: 0.8047722578\n",
      "\tVALIDATION\tLOSS: 1.5670280457\tACC: 0.7083333135\n",
      "EP: 141\tLOSS: 1.4714279175\tACC: 0.7982646227\n",
      "EP: 142\tLOSS: 1.4872243404\tACC: 0.7852494717\n",
      "EP: 143\tLOSS: 1.4832521677\tACC: 0.7939262390\n",
      "EP: 144\tLOSS: 1.4831520319\tACC: 0.7895878553\n",
      "EP: 145\tLOSS: 1.4652637243\tACC: 0.8069414496\n",
      "EP: 146\tLOSS: 1.4771020412\tACC: 0.7982646227\n",
      "EP: 147\tLOSS: 1.4800651073\tACC: 0.8091106415\n",
      "EP: 148\tLOSS: 1.4778106213\tACC: 0.8069414496\n",
      "EP: 149\tLOSS: 1.4716124535\tACC: 0.8047722578\n",
      "EP: 150\tLOSS: 1.4656574726\tACC: 0.8177874088\n",
      "\tVALIDATION\tLOSS: 1.5212283134\tACC: 0.7583333254\n",
      "EP: 151\tLOSS: 1.4603358507\tACC: 0.8112798333\n",
      "EP: 152\tLOSS: 1.4532825947\tACC: 0.8221257925\n",
      "EP: 153\tLOSS: 1.4659315348\tACC: 0.8069414496\n",
      "EP: 154\tLOSS: 1.4449557066\tACC: 0.8351410031\n",
      "EP: 155\tLOSS: 1.4485207796\tACC: 0.8286334276\n",
      "EP: 156\tLOSS: 1.4512318373\tACC: 0.8286334276\n",
      "EP: 157\tLOSS: 1.4373519421\tACC: 0.8438177705\n",
      "EP: 158\tLOSS: 1.4639047384\tACC: 0.8091106415\n",
      "EP: 159\tLOSS: 1.4415398836\tACC: 0.8329718113\n",
      "EP: 160\tLOSS: 1.4519000053\tACC: 0.8221257925\n",
      "\tVALIDATION\tLOSS: 1.4909808636\tACC: 0.7916666865\n",
      "EP: 161\tLOSS: 1.4494720697\tACC: 0.8221257925\n",
      "EP: 162\tLOSS: 1.4195922613\tACC: 0.8568329811\n",
      "EP: 163\tLOSS: 1.4311239719\tACC: 0.8438177705\n",
      "EP: 164\tLOSS: 1.4483807087\tACC: 0.8286334276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP: 165\tLOSS: 1.4446377754\tACC: 0.8394793868\n",
      "EP: 166\tLOSS: 1.4442954063\tACC: 0.8329718113\n",
      "EP: 167\tLOSS: 1.4369292259\tACC: 0.8373101950\n",
      "EP: 168\tLOSS: 1.4355489016\tACC: 0.8438177705\n",
      "EP: 169\tLOSS: 1.4386259317\tACC: 0.8438177705\n",
      "EP: 170\tLOSS: 1.4332518578\tACC: 0.8416485786\n",
      "\tVALIDATION\tLOSS: 1.4825416803\tACC: 0.7916666865\n",
      "EP: 171\tLOSS: 1.4266726971\tACC: 0.8568329811\n",
      "EP: 172\tLOSS: 1.4349930286\tACC: 0.8459869623\n",
      "EP: 173\tLOSS: 1.4345687628\tACC: 0.8416485786\n",
      "EP: 174\tLOSS: 1.4323545694\tACC: 0.8416485786\n",
      "EP: 175\tLOSS: 1.4077625275\tACC: 0.8698481321\n",
      "EP: 176\tLOSS: 1.4030251503\tACC: 0.8785249591\n",
      "EP: 177\tLOSS: 1.4351207018\tACC: 0.8329718113\n",
      "EP: 178\tLOSS: 1.4311336279\tACC: 0.8459869623\n",
      "EP: 179\tLOSS: 1.4235153198\tACC: 0.8524945974\n",
      "EP: 180\tLOSS: 1.4258950949\tACC: 0.8524945974\n",
      "\tVALIDATION\tLOSS: 1.4968678951\tACC: 0.7749999762\n",
      "EP: 181\tLOSS: 1.4207015038\tACC: 0.8524945974\n",
      "EP: 182\tLOSS: 1.4201316833\tACC: 0.8481561542\n",
      "EP: 183\tLOSS: 1.4175926447\tACC: 0.8568329811\n",
      "EP: 184\tLOSS: 1.4104760885\tACC: 0.8676789403\n",
      "EP: 185\tLOSS: 1.4133532047\tACC: 0.8611713648\n",
      "EP: 186\tLOSS: 1.4036235809\tACC: 0.8698481321\n",
      "EP: 187\tLOSS: 1.4321135283\tACC: 0.8373101950\n",
      "EP: 188\tLOSS: 1.4029644728\tACC: 0.8763557673\n",
      "EP: 189\tLOSS: 1.4493457079\tACC: 0.8221257925\n",
      "EP: 190\tLOSS: 1.4130856991\tACC: 0.8633405566\n",
      "\tVALIDATION\tLOSS: 1.4721242189\tACC: 0.8083333373\n",
      "EP: 191\tLOSS: 1.3938314915\tACC: 0.8806941509\n",
      "EP: 192\tLOSS: 1.3911411762\tACC: 0.8828633428\n",
      "EP: 193\tLOSS: 1.3999248743\tACC: 0.8763557673\n",
      "EP: 194\tLOSS: 1.4085381031\tACC: 0.8676789403\n",
      "EP: 195\tLOSS: 1.3982497454\tACC: 0.8828633428\n",
      "EP: 196\tLOSS: 1.4166103601\tACC: 0.8590021729\n",
      "EP: 197\tLOSS: 1.4008204937\tACC: 0.8915401101\n",
      "EP: 198\tLOSS: 1.3930528164\tACC: 0.8828633428\n",
      "EP: 199\tLOSS: 1.4016609192\tACC: 0.8720173240\n",
      "EP: 200\tLOSS: 1.4137266874\tACC: 0.8590021729\n",
      "\tVALIDATION\tLOSS: 1.4625834227\tACC: 0.8166666627\n",
      "EP: 201\tLOSS: 1.3939104080\tACC: 0.8806941509\n",
      "EP: 202\tLOSS: 1.4036858082\tACC: 0.8676789403\n",
      "EP: 203\tLOSS: 1.4065548182\tACC: 0.8698481321\n",
      "EP: 204\tLOSS: 1.3871595860\tACC: 0.8872017264\n",
      "EP: 205\tLOSS: 1.4015414715\tACC: 0.8741865754\n",
      "EP: 206\tLOSS: 1.3890216351\tACC: 0.8850325346\n",
      "EP: 207\tLOSS: 1.3892846107\tACC: 0.8828633428\n",
      "EP: 208\tLOSS: 1.3859144449\tACC: 0.8872017264\n",
      "EP: 209\tLOSS: 1.3915952444\tACC: 0.8850325346\n",
      "EP: 210\tLOSS: 1.3901855946\tACC: 0.8850325346\n",
      "\tVALIDATION\tLOSS: 1.4604957104\tACC: 0.8166666627\n",
      "EP: 211\tLOSS: 1.4023624659\tACC: 0.8785249591\n",
      "EP: 212\tLOSS: 1.3771466017\tACC: 0.9002169371\n",
      "EP: 213\tLOSS: 1.3830926418\tACC: 0.8958785534\n",
      "EP: 214\tLOSS: 1.3939911127\tACC: 0.8850325346\n",
      "EP: 215\tLOSS: 1.3916321993\tACC: 0.8828633428\n",
      "EP: 216\tLOSS: 1.3894064426\tACC: 0.8893709183\n",
      "EP: 217\tLOSS: 1.3780257702\tACC: 0.9023861289\n",
      "EP: 218\tLOSS: 1.3938958645\tACC: 0.8785249591\n",
      "EP: 219\tLOSS: 1.3869715929\tACC: 0.8893709183\n",
      "EP: 220\tLOSS: 1.3855878115\tACC: 0.8937093019\n",
      "\tVALIDATION\tLOSS: 1.4396469593\tACC: 0.8416666389\n",
      "EP: 221\tLOSS: 1.3734202385\tACC: 0.9067245126\n",
      "EP: 222\tLOSS: 1.3769559860\tACC: 0.9002169371\n",
      "EP: 223\tLOSS: 1.3927403688\tACC: 0.8785249591\n",
      "EP: 224\tLOSS: 1.3753314018\tACC: 0.9045553207\n",
      "EP: 225\tLOSS: 1.3611699343\tACC: 0.9154012799\n",
      "EP: 226\tLOSS: 1.3786575794\tACC: 0.9023861289\n",
      "EP: 227\tLOSS: 1.3689184189\tACC: 0.9067245126\n",
      "EP: 228\tLOSS: 1.3802429438\tACC: 0.8958785534\n",
      "EP: 229\tLOSS: 1.3680430651\tACC: 0.9088937044\n",
      "EP: 230\tLOSS: 1.3639172316\tACC: 0.9088937044\n",
      "\tVALIDATION\tLOSS: 1.4686473608\tACC: 0.7916666865\n",
      "EP: 231\tLOSS: 1.3859188557\tACC: 0.8958785534\n",
      "EP: 232\tLOSS: 1.3891909122\tACC: 0.8872017264\n",
      "EP: 233\tLOSS: 1.3819196224\tACC: 0.8958785534\n",
      "EP: 234\tLOSS: 1.3684337139\tACC: 0.9067245126\n",
      "EP: 235\tLOSS: 1.3989975452\tACC: 0.8698481321\n",
      "EP: 236\tLOSS: 1.3774840832\tACC: 0.9002169371\n",
      "EP: 237\tLOSS: 1.3892214298\tACC: 0.8828633428\n",
      "EP: 238\tLOSS: 1.3727580309\tACC: 0.9067245126\n",
      "EP: 239\tLOSS: 1.3794745207\tACC: 0.9002169371\n",
      "EP: 240\tLOSS: 1.3810391426\tACC: 0.8937093019\n",
      "\tVALIDATION\tLOSS: 1.4497967958\tACC: 0.8166666627\n",
      "EP: 241\tLOSS: 1.3753342628\tACC: 0.9002169371\n",
      "EP: 242\tLOSS: 1.3841700554\tACC: 0.8872017264\n",
      "EP: 243\tLOSS: 1.3570209742\tACC: 0.9219089150\n",
      "EP: 244\tLOSS: 1.3748352528\tACC: 0.8980477452\n",
      "EP: 245\tLOSS: 1.3899519444\tACC: 0.8850325346\n",
      "EP: 246\tLOSS: 1.3646074533\tACC: 0.9088937044\n",
      "EP: 247\tLOSS: 1.3737754822\tACC: 0.9023861289\n",
      "EP: 248\tLOSS: 1.3643510342\tACC: 0.9067245126\n",
      "EP: 249\tLOSS: 1.3727153540\tACC: 0.9088937044\n",
      "EP: 250\tLOSS: 1.3723750114\tACC: 0.9002169371\n",
      "\tVALIDATION\tLOSS: 1.4676787853\tACC: 0.8000000119\n",
      "EP: 251\tLOSS: 1.3752857447\tACC: 0.9023861289\n",
      "EP: 252\tLOSS: 1.3591469526\tACC: 0.9154012799\n",
      "EP: 253\tLOSS: 1.3724548817\tACC: 0.9002169371\n",
      "EP: 254\tLOSS: 1.3570255041\tACC: 0.9219089150\n",
      "EP: 255\tLOSS: 1.3753311634\tACC: 0.9002169371\n",
      "EP: 256\tLOSS: 1.3706691265\tACC: 0.9023861289\n",
      "EP: 257\tLOSS: 1.3784034252\tACC: 0.8958785534\n",
      "EP: 258\tLOSS: 1.3726007938\tACC: 0.9045553207\n",
      "EP: 259\tLOSS: 1.3643859625\tACC: 0.9110628963\n",
      "EP: 260\tLOSS: 1.3655133247\tACC: 0.9132320881\n",
      "\tVALIDATION\tLOSS: 1.5037127733\tACC: 0.7666666508\n",
      "EP: 261\tLOSS: 1.3650405407\tACC: 0.9132320881\n",
      "EP: 262\tLOSS: 1.3596899509\tACC: 0.9175704718\n",
      "EP: 263\tLOSS: 1.3659329414\tACC: 0.9088937044\n",
      "EP: 264\tLOSS: 1.3691523075\tACC: 0.9045553207\n",
      "EP: 265\tLOSS: 1.3569420576\tACC: 0.9197397232\n",
      "EP: 266\tLOSS: 1.3611497879\tACC: 0.9154012799\n",
      "EP: 267\tLOSS: 1.3707094193\tACC: 0.9045553207\n",
      "EP: 268\tLOSS: 1.3669601679\tACC: 0.9088937044\n",
      "EP: 269\tLOSS: 1.3594614267\tACC: 0.9154012799\n",
      "EP: 270\tLOSS: 1.3602482080\tACC: 0.9219089150\n",
      "\tVALIDATION\tLOSS: 1.4562708139\tACC: 0.8166666627\n",
      "EP: 271\tLOSS: 1.3472524881\tACC: 0.9305856824\n",
      "EP: 272\tLOSS: 1.3559533358\tACC: 0.9240781069\n",
      "EP: 273\tLOSS: 1.3562862873\tACC: 0.9154012799\n",
      "EP: 274\tLOSS: 1.3612086773\tACC: 0.9088937044\n",
      "EP: 275\tLOSS: 1.3444797993\tACC: 0.9305856824\n",
      "EP: 276\tLOSS: 1.3918828964\tACC: 0.8741865754\n",
      "EP: 277\tLOSS: 1.3694027662\tACC: 0.9023861289\n",
      "EP: 278\tLOSS: 1.3441753387\tACC: 0.9349240661\n",
      "EP: 279\tLOSS: 1.3708926439\tACC: 0.9067245126\n",
      "EP: 280\tLOSS: 1.3862500191\tACC: 0.8828633428\n",
      "\tVALIDATION\tLOSS: 1.4890162945\tACC: 0.7833333611\n",
      "EP: 281\tLOSS: 1.3895941973\tACC: 0.8872017264\n",
      "EP: 282\tLOSS: 1.3562430143\tACC: 0.9154012799\n",
      "EP: 283\tLOSS: 1.3309072256\tACC: 0.9436008930\n",
      "EP: 284\tLOSS: 1.3668270111\tACC: 0.9088937044\n",
      "EP: 285\tLOSS: 1.3797534704\tACC: 0.8915401101\n",
      "EP: 286\tLOSS: 1.3575903177\tACC: 0.9175704718\n",
      "EP: 287\tLOSS: 1.3694496155\tACC: 0.9045553207\n",
      "EP: 288\tLOSS: 1.3408081532\tACC: 0.9327548742\n",
      "EP: 289\tLOSS: 1.3433237076\tACC: 0.9370932579\n",
      "EP: 290\tLOSS: 1.3645014763\tACC: 0.9154012799\n",
      "\tVALIDATION\tLOSS: 1.4490658045\tACC: 0.8166666627\n",
      "EP: 291\tLOSS: 1.3446058035\tACC: 0.9370932579\n",
      "EP: 292\tLOSS: 1.3554559946\tACC: 0.9240781069\n",
      "EP: 293\tLOSS: 1.3498766422\tACC: 0.9262472987\n",
      "EP: 294\tLOSS: 1.3471275568\tACC: 0.9327548742\n",
      "EP: 295\tLOSS: 1.3482497931\tACC: 0.9305856824\n",
      "EP: 296\tLOSS: 1.3634202480\tACC: 0.9132320881\n",
      "EP: 297\tLOSS: 1.3460700512\tACC: 0.9327548742\n",
      "EP: 298\tLOSS: 1.3367804289\tACC: 0.9392624497\n",
      "EP: 299\tLOSS: 1.3476659060\tACC: 0.9219089150\n"
     ]
    }
   ],
   "source": [
    "m.train(dat['train'], dat['test'], 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
